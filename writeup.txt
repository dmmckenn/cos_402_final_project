	The algorithms we implemented were Adaboost using weighted decision stumps, and naïve Bayes classification. Decision stumps were implemented in Stumps.java. The decision stumps algorithm uses a binary data set, and uses the best attribute in the data to split on. Each attribute is weighted from outside of the stumps implementation, and the best attribute is defined as the one which maximizes the absolute value of weights for a given output. We use absolute value because the strategy could be positive (i.e. use the same direction as the attribute for the output) or negative (use the opposite direction). The Adaboost implementation uses the vanilla Adaboost algorithm described in the text, updating weights on each iteration to rectify badly classified samples, and feeding these weights into the weighted stumps algorithm. Our second algorithm, naïve Bayes, was also implemented using the vanilla implementation described in the text.
	We used a few different strategies to test if the program was working properly. First, we designed our own data set to test the data. The data set we checked on was that corresponding to Boolean tests of dice roles. So the attributes were, in one case, an array of dice values (1 to 6) and the output was whether the roll was greater than 3 (strictly). We could generate large test sets programmatically (and training sets), and verified Ada+stumps and NB this way. Stumps alone was verified similarly, but with four attributes – the first three corresponding to rolls of 1, 2, or 3 respectively, and the last to a roll greater than 3. Stumps worked correctly, splitting positively on the last attribute. 
	We also tested our algorithms by comparing to other students on the submission page. As we ranked toward the top on each of the four data sets, we felt confident we had implemented our algorithms correctly. Furthermore, our results tended to be closely clustered around others who described using very similar algorithms. For example, most of our Adaboost results ended up near other who also used Adaboost with decision stumps.
	The experiments we carried out included running tests on Adaboost with stumps, and naïve Bayes by using an increasing number of training examples and noting the accuracy of the results each time. In other words, we started with 20 examples, and then doubled the number of examples each time until reaching roughly the number of training examples available. We did this for each of the four data sets provided.

	Our algorithms seemed to work reasonably well on each of the four provided data sets. For the census data, our Adaboost with stumps algorithm tied for fifth place in the class, with an error rate of 18.2%. Naïve Bayes also performed well, in the upper third of class results, with an error rate of 19.4%. For the DNA data set, naïve Bayes performed better than the Adaboost algorithm, with an error rate of 8.8%, better than many of our classmates, but not near the top (which was around 2%). Adaboost performed in the middle of the pack here, with an error rate of 9%. For OCR17, Adaboost again tied for fourth place, with 0.7% error. Naïve Bayes performed in the second half of the class, with an error rate of 9.4%. For OCR49, Adaboost performed in the top half of the class with an error rate of 5.9%, while naïve Bayes performed somewhat poorly with an error rate of 13.5%. Overall, we can conclude that the Adaboost algorithm with weighted stumps performs better than naïve Bayes, but that both work reasonably well. Overall, Adaboost seemed to be competitive with the top algorithms in the class, shining especially on the census data case.
